import json
import os
from neo4j import GraphDatabase
import numpy as np
from llm import llm
from collections import Counter
from sentence_transformers import SentenceTransformer

# --- Neo4j ì—°ê²° ì •ë³´ ---
URI = "bolt://localhost:7687"
AUTH = ("neo4j", "PW")
DATABASE = "neo4j"
driver = GraphDatabase.driver(URI, auth=AUTH)

PROMPT_FILE = "C:\\Users\\KISIA\\Desktop\\Enki\\Neo4j\\summary_prompt.md"

embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')


def create_summary_context(trace_data):
    context_lines = []

    def get_tag_value(tags, key, default=None):
        for tag in tags:
            if tag.get('key') == key:
                return tag.get('value')
        return default

    sigma_alerts = Counter()
    process_flows = Counter()
    network_events = Counter()
    file_events = Counter()
    registry_events = Counter()

    for span in trace_data.get('spans', []):
        tags = span.get('tags', [])
        event_name = get_tag_value(tags, 'EventName', '')
        process_image = get_tag_value(tags, 'Image')
        process_name = os.path.basename(process_image or 'N/A')

        # Sigma ë£° íƒì§€
        rule_title = get_tag_value(tags, 'sigma.rule_title')
        if rule_title:
            sigma_alerts[f"ê·œì¹™: {rule_title}, í”„ë¡œì„¸ìŠ¤: {process_name}"] += 1

        # í”„ë¡œì„¸ìŠ¤ ìƒì„± íë¦„
        if "ProcessCreate" in event_name:
            parent_image = get_tag_value(tags, 'ParentImage')
            if parent_image and process_image:
                process_flows[f"'{os.path.basename(parent_image)}'ê°€ '{process_name}'ë¥¼ ì‹¤í–‰"] += 1

        # ë„¤íŠ¸ì›Œí¬/íŒŒì¼/ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì´ë²¤íŠ¸
        if "NetworkConnect" in event_name:
            dest_ip = get_tag_value(tags, 'DestinationIp')
            dest_port = get_tag_value(tags, 'DestinationPort')
            if dest_ip and dest_port:
                network_events[f"[ë„¤íŠ¸ì›Œí¬] '{process_name}'ê°€ '{dest_ip}:{dest_port}'ë¡œ ì—°ê²°"] += 1
        elif "FileCreate" in event_name:
            target_file = get_tag_value(tags, 'TargetFilename')
            if target_file:
                file_events[f"[íŒŒì¼] '{process_name}'ê°€ '{target_file}' íŒŒì¼ì„ ìƒì„±"] += 1
        elif "RegistryValueSet" in event_name:
            target_object = get_tag_value(tags, 'TargetObject')
            if target_object:
                registry_events[f"[ë ˆì§€ìŠ¤íŠ¸ë¦¬] '{process_name}'ê°€ '{target_object}' í‚¤ ê°’ì„ ìˆ˜ì •"] += 1

    # ì»¨í…ìŠ¤íŠ¸ ìƒì„±
    if sigma_alerts:
        context_lines.append("### Sigma Rule íƒì§€ ìš”ì•½:")
        for item, count in sigma_alerts.most_common():
            context_lines.append(f"- {item} ({count}íšŒ)")
    if process_flows:
        context_lines.append("\n### ì£¼ìš” í”„ë¡œì„¸ìŠ¤ ìƒì„± íë¦„:")
        for item, count in process_flows.most_common():
            context_lines.append(f"- {item} ({count}íšŒ)")
    if network_events or file_events or registry_events:
        context_lines.append("\n### ê¸°íƒ€ ì£¼ìš” ì´ë²¤íŠ¸:")
        for item, count in network_events.most_common(5):
            context_lines.append(f"- {item} ({count}íšŒ)")
        for item, count in file_events.most_common(5):
            context_lines.append(f"- {item} ({count}íšŒ)")
        for item, count in registry_events.most_common(5):
            context_lines.append(f"- {item} ({count}íšŒ)")

    return "\n".join(context_lines)


def summarize_trace_with_llm(trace_input, prompt_template):
    if isinstance(trace_input, str):
        with open(trace_input, 'r', encoding='utf-8-sig') as f:
            trace_data = json.load(f)
    else:
        trace_data = trace_input

    summary_context = create_summary_context(trace_data)
    final_prompt = prompt_template.replace("[ë¶„ì„í•  JSON ë°ì´í„°ê°€ ì—¬ê¸°ì— ì‚½ì…ë©ë‹ˆë‹¤]", summary_context)

    try:
        response = llm.invoke(final_prompt)
        raw_content = response.content
        if not raw_content.strip():
            return {"error": "LLMìœ¼ë¡œë¶€í„° ë¹ˆ ì‘ë‹µì„ ë°›ì•˜ìŠµë‹ˆë‹¤."}

        cleaned_content = raw_content.strip()
        if cleaned_content.startswith("```json"):
            cleaned_content = cleaned_content.split('\n', 1)[1]
        if cleaned_content.endswith("```"):
            cleaned_content = cleaned_content.rsplit('\n', 1)[0]

        analysis_result = json.loads(cleaned_content.strip())
        return analysis_result
    except json.JSONDecodeError:
        return {"error": "LLMì´ ìœ íš¨í•œ JSONì„ ë°˜í™˜í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", "raw_response": raw_content}
    except Exception as e:
        return {"error": f"LLM í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"}


def cosine_similarity(vec1, vec2):
    v1 = np.array(vec1, dtype=float)
    v2 = np.array(vec2, dtype=float)
    if np.linalg.norm(v1) == 0 or np.linalg.norm(v2) == 0:
        return 0.0
    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))


def find_similar_traces(driver, summary_text, top_k=5):
    with driver.session(database=DATABASE) as session:
        all_summaries = session.run("""
            MATCH (t:Trace)-[:HAS_SUMMARY]->(s:Summary)
            RETURN 
                coalesce(t.traceId, t.`traceId:ID(Trace)`) AS trace_id, 
                s.embedding AS embedding
        """)

        summary_embedding = embedding_model.encode(summary_text)
        similarities = []

        for record in all_summaries:
            trace_id = record['trace_id']
            emb = record['embedding']
            sim = cosine_similarity(summary_embedding, emb)
            similarities.append({'trace_id': trace_id, 'similarity': sim})

        similarities.sort(key=lambda x: x['similarity'], reverse=True)
        return similarities[:top_k]


def generate_mitigation_prompt(summary_result, structural_similarity, indirect_connections):
    """
    LLMì—ê²Œ ì•…ì„± í–‰ìœ„ ëŒ€ì‘ ë°©ì•ˆì„ ìš”ì²­í•˜ëŠ” í”„ë¡¬í”„íŠ¸ ìƒì„±
    """
    summary_text = summary_result.get('summary', '')

    similar_entities = set()
    similar_techniques = set()
    for s in structural_similarity:
        similar_entities.update(s['common_entities'])
        similar_techniques.update(s['common_techniques'])

    for c in indirect_connections:
        similar_entities.add(c['e1_name'])
        similar_entities.add(c['e2_name'])

    prompt = f"""
    ë‹¹ì‹ ì€ ë³´ì•ˆ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ íŠ¸ë ˆì´ìŠ¤ ë¶„ì„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê¸°ì—… í™˜ê²½ì—ì„œ ë°œê²¬ëœ ì•…ì„± í–‰ìœ„ì— ëŒ€í•œ 
    ì‹¤ì œ ëŒ€ì‘ ë°©ì•ˆì„ êµ¬ì²´ì ìœ¼ë¡œ ì œì•ˆí•´ì£¼ì„¸ìš”.

    [íŠ¸ë ˆì´ìŠ¤ ìš”ì•½]
    {summary_text}

    [ì—°ê´€ ì—”í‹°í‹°]
    {', '.join(similar_entities)}

    [ì—°ê´€ ê³µê²© ê¸°ìˆ  / TTP]
    {', '.join(similar_techniques)}

    [ìš”ì²­]
    1. íƒì§€ëœ ì•…ì„± í”„ë¡œì„¸ìŠ¤ ë° íŒŒì¼ ê²©ë¦¬ ë°©ë²•
    2. ë„¤íŠ¸ì›Œí¬ ì°¨ë‹¨ ë° ì™¸ë¶€ í†µì‹  í†µì œ ë°©ì•ˆ
    3. ë¡œê·¸/ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ ê°•í™” ë°©ë²•
    4. í–¥í›„ ìœ ì‚¬ ê³µê²© ì˜ˆë°© ì „ëµ
    5. ì‹¤ë¬´ìê°€ ë°”ë¡œ ì ìš© ê°€ëŠ¥í•œ ë‹¨ê³„ë³„ ëŒ€ì‘ ê¶Œì¥

    ì‘ë‹µì€ JSON ë˜ëŠ” ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ê³ , ë‹¨ê³„ë³„ë¡œ ë²ˆí˜¸ë¥¼ ë¶™ì—¬ ìƒì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”.
    ì–¸ì–´ëŠ” ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ë‹µí•˜ì„¸ìš”.
    """
    return prompt


def analyze_structural_similarity_no_db(driver, new_trace, prompt_template, top_k=5):
    # LLM ìš”ì•½
    summary_result = summarize_trace_with_llm(new_trace, prompt_template)
    if 'error' in summary_result:
        return summary_result
    summary_text = summary_result.get('summary', '')

    # ì˜ë¯¸ì  ìœ ì‚¬ íŠ¸ë ˆì´ìŠ¤ ê²€ìƒ‰
    top_similar_traces = find_similar_traces(driver, summary_text, top_k=top_k)
    similar_ids = [t['trace_id'] for t in top_similar_traces]

    print(f"\nğŸ” ì˜ë¯¸ì  ìœ ì‚¬ë„ ìƒìœ„ {top_k} íŠ¸ë ˆì´ìŠ¤: {similar_ids}\n")

    # êµ¬ì¡°ì  ìœ ì‚¬ì„± ë¶„ì„
    with driver.session(database=DATABASE) as session:
        res = session.run("""
            MATCH (t:Trace)-[:HAS_SUMMARY]->(s:Summary)
            WHERE t.traceId IN $trace_ids
            OPTIONAL MATCH (s)-[:MENTIONS]->(e)
            OPTIONAL MATCH (s)-[:INDICATES_TECHNIQUE]->(tech)
            RETURN t.traceId AS trace_id, 
                   collect(DISTINCT coalesce(e.name, e.filePath, e.address, e.keyPath)) AS entities,
                   collect(DISTINCT tech.name) AS techniques
        """, trace_ids=similar_ids)

        trace_json = new_trace if isinstance(new_trace, dict) else json.load(open(new_trace, 'r', encoding='utf-8-sig'))
        new_entities = set([e['value'] for e in trace_json.get('key_entities', [])])
        new_techniques = set([t['name'] for t in trace_json.get('attack_techniques', [])])

        comparisons = []
        for record in res:
            db_entities = set(record['entities'])
            db_techniques = set(record['techniques'])
            common_entities = new_entities & db_entities
            common_techniques = new_techniques & db_techniques
            comparisons.append({
                'trace_id': record['trace_id'],
                'common_entities': list(common_entities),
                'common_techniques': list(common_techniques),
                'entity_match_count': len(common_entities),
                'technique_match_count': len(common_techniques)
            })

        comparisons.sort(key=lambda x: (x['entity_match_count'], x['technique_match_count']), reverse=True)

    # ê°„ì ‘ ì—°ê²° íƒìƒ‰
    with driver.session(database=DATABASE) as session:
        query = """
            UNWIND $trace_ids AS trace_id
            MATCH (t:Trace {traceId: trace_id})-[:HAS_SUMMARY]->(:Summary)-[:MENTIONS]->(e)
            WITH collect(DISTINCT e) AS groupEntities
            UNWIND groupEntities AS e1
            UNWIND groupEntities AS e2
            WITH e1, e2 WHERE id(e1) < id(e2)
            MATCH path = shortestPath((e1)-[*..2]-(e2))
            RETURN e1.name AS e1_name, e2.name AS e2_name,
                   length(path) AS hops,
                   [n IN nodes(path) | labels(n)[0] + ':' + coalesce(n.name, n.filePath, n.address, '')] AS path_nodes
            LIMIT 50
        """
        indirect_connections = session.run(query, trace_ids=similar_ids)
        indirect_connections = [r.data() for r in indirect_connections]

    # ëŒ€ì‘ ì œì•ˆ ìƒì„±
    mitigation_prompt = generate_mitigation_prompt(summary_result, comparisons, indirect_connections)
    mitigation_response = llm.invoke(mitigation_prompt)

    return {
        'summary': summary_result,
        'semantic_top_traces': top_similar_traces,
        'structural_similarity': comparisons,
        'indirect_connections': indirect_connections,
        'mitigation_suggestions': mitigation_response.content
    }


if __name__ == "__main__":
    # ìš”ì•½ í”„ë¡¬í”„íŠ¸ ì½ê¸°
    try:
        with open(PROMPT_FILE, 'r', encoding='utf-8') as f:
            prompt_template = f.read()
    except FileNotFoundError:
        print(f"ì˜¤ë¥˜: '{os.path.abspath(PROMPT_FILE)}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        exit()

    # ë¶„ì„í•  trace ê²½ë¡œ -> kafka ì—°ë™ í•„ìš”
    trace_path = "C:\\Users\\KISIA\\Downloads\\data\\~trace-0c19072a66a94fe548636d7b50a06bef.json"

    results = analyze_structural_similarity_no_db(driver, trace_path, prompt_template, top_k=5)
    print(json.dumps(results, ensure_ascii=False, indent=2))
